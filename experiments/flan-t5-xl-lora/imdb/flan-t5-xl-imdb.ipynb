{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Finetuning the model on financial_phrasebank dataset, that consists of pairs of text-labels to classify financial-related sentences, if they are either <span style=\"color: red;\">positive</span>, <span style=\"color: purple;\">neutral</span> or <span style=\"color: green;\">negative</span>."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 1.Experimental Setup1"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1.1 Setup"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1.2 Training"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1.2.1 Run code on CPU version"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!pwd"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the module from /Users/wengbenjue/.cache/huggingface/modules/evaluate_modules/metrics/evaluate-metric--accuracy/f887c0aab52c2d38e1f8a215681126379eca617f96c447638f751434e8e65b14 (last modified on Mon Apr 24 01:17:15 2023) since it couldn't be found locally at evaluate-metric--accuracy, or remotely on the Hugging Face Hub.\r\n",
      "use AutoModelForSeq2SeqLM load bert model.\r\n",
      "trainable params: 2752512 || all params: 79713664 || trainable%: 3.4529989739274813\r\n",
      "Running tokenizer on dataset: 100%|█| 4075/4075 [00:00<00:00, 24174.06 examples/\r\n",
      "Running tokenizer on dataset: 100%|█| 453/453 [00:00<00:00, 22778.73 examples/s]\r\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\r\n",
      "To disable this warning, you can either:\r\n",
      "\t- Avoid using `tokenizers` before the fork if possible\r\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\r\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\r\n",
      "To disable this warning, you can either:\r\n",
      "\t- Avoid using `tokenizers` before the fork if possible\r\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\r\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\r\n",
      "To disable this warning, you can either:\r\n",
      "\t- Avoid using `tokenizers` before the fork if possible\r\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\r\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\r\n",
      "To disable this warning, you can either:\r\n",
      "\t- Avoid using `tokenizers` before the fork if possible\r\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\r\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: Currently logged in as: \u001B[33mwengbenjue\u001B[0m. Use \u001B[1m`wandb login --relogin`\u001B[0m to force relogin\r\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: Tracking run with wandb version 0.16.0\r\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: Run data is saved locally in \u001B[35m\u001B[1m/Users/wengbenjue/sourcecode/peft/llms-peft-cook/experiments/flan-t5-small/financial_phrasebank/wandb/run-20231201_204434-klb4l5uj\u001B[0m\r\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: Run \u001B[1m`wandb offline`\u001B[0m to turn off syncing.\r\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: Syncing run \u001B[33mkind-sky-39\u001B[0m\r\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: ⭐️ View project at \u001B[34m\u001B[4mhttps://wandb.ai/wengbenjue/huggingface\u001B[0m\r\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: 🚀 View run at \u001B[34m\u001B[4mhttps://wandb.ai/wengbenjue/huggingface/runs/klb4l5uj\u001B[0m\r\n",
      "  0%|                                                  | 0/1270 [00:00<?, ?it/s]/Users/wengbenjue/opt/anaconda3/envs/peft/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\r\n",
      "  warnings.warn(\r\n",
      "{'loss': 3.6778, 'learning_rate': 0.00019842519685039372, 'epoch': 0.04}        \r\n",
      "{'loss': 3.646, 'learning_rate': 0.00019685039370078743, 'epoch': 0.08}         \r\n",
      "{'loss': 3.395, 'learning_rate': 0.0001952755905511811, 'epoch': 0.12}          \r\n",
      "{'loss': 2.9518, 'learning_rate': 0.0001937007874015748, 'epoch': 0.16}         \r\n",
      "{'loss': 2.7517, 'learning_rate': 0.0001921259842519685, 'epoch': 0.2}          \r\n",
      "{'loss': 2.3924, 'learning_rate': 0.0001905511811023622, 'epoch': 0.24}         \r\n",
      "{'loss': 2.0975, 'learning_rate': 0.0001889763779527559, 'epoch': 0.27}         \r\n",
      "{'loss': 1.5977, 'learning_rate': 0.00018740157480314962, 'epoch': 0.31}        \r\n",
      "{'loss': 1.145, 'learning_rate': 0.00018582677165354333, 'epoch': 0.35}         \r\n",
      "{'loss': 0.6939, 'learning_rate': 0.000184251968503937, 'epoch': 0.39}          \r\n",
      "  8%|███▏                                    | 100/1270 [01:45<19:15,  1.01it/s]/Users/wengbenjue/opt/anaconda3/envs/peft/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\r\n",
      "  warnings.warn(\r\n",
      "{'loss': 0.3618, 'learning_rate': 0.00018267716535433072, 'epoch': 0.43}        \r\n",
      "{'loss': 0.2326, 'learning_rate': 0.0001811023622047244, 'epoch': 0.47}         \r\n",
      "{'loss': 0.2509, 'learning_rate': 0.0001795275590551181, 'epoch': 0.51}         \r\n",
      "{'loss': 0.2332, 'learning_rate': 0.00017795275590551182, 'epoch': 0.55}        \r\n",
      "{'loss': 0.197, 'learning_rate': 0.00017637795275590552, 'epoch': 0.59}         \r\n",
      "{'loss': 0.1979, 'learning_rate': 0.00017480314960629923, 'epoch': 0.63}        \r\n",
      "{'loss': 0.1967, 'learning_rate': 0.0001732283464566929, 'epoch': 0.67}         \r\n",
      "{'loss': 0.214, 'learning_rate': 0.00017165354330708662, 'epoch': 0.71}         \r\n",
      "{'loss': 0.18, 'learning_rate': 0.00017007874015748033, 'epoch': 0.75}          \r\n",
      "{'loss': 0.1925, 'learning_rate': 0.000168503937007874, 'epoch': 0.79}          \r\n",
      " 16%|██████▎                                 | 200/1270 [03:26<17:54,  1.00s/it]/Users/wengbenjue/opt/anaconda3/envs/peft/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\r\n",
      "  warnings.warn(\r\n",
      "{'loss': 0.1854, 'learning_rate': 0.00016692913385826772, 'epoch': 0.82}        \r\n",
      "{'loss': 0.1664, 'learning_rate': 0.00016535433070866143, 'epoch': 0.86}        \r\n",
      " 17%|██████▉                                 | 222/1270 [03:48<17:36,  1.01s/it]\u001B[34m\u001B[1mwandb\u001B[0m: Network error (TransientError), entering retry loop.\r\n",
      "{'loss': 0.2183, 'learning_rate': 0.00016377952755905514, 'epoch': 0.9}         \r\n",
      "{'loss': 0.1613, 'learning_rate': 0.00016220472440944882, 'epoch': 0.94}        \r\n",
      "{'loss': 0.1822, 'learning_rate': 0.00016062992125984252, 'epoch': 0.98}        \r\n",
      " 20%|████████                                | 254/1270 [04:21<16:40,  1.02it/s]\r\n",
      "  0%|                                                    | 0/57 [00:00<?, ?it/s]\u001B[A\r\n",
      "  4%|█▌                                          | 2/57 [00:00<00:05, 10.16it/s]\u001B[A\r\n",
      "  7%|███                                         | 4/57 [00:00<00:08,  6.47it/s]\u001B[A\r\n",
      "  9%|███▊                                        | 5/57 [00:00<00:08,  6.04it/s]\u001B[A\r\n",
      " 11%|████▋                                       | 6/57 [00:00<00:08,  5.72it/s]\u001B[A\r\n",
      " 12%|█████▍                                      | 7/57 [00:01<00:08,  5.59it/s]\u001B[A\r\n",
      " 14%|██████▏                                     | 8/57 [00:01<00:09,  5.43it/s]\u001B[A\r\n",
      " 16%|██████▉                                     | 9/57 [00:01<00:08,  5.41it/s]\u001B[A\r\n",
      " 18%|███████▌                                   | 10/57 [00:01<00:08,  5.32it/s]\u001B[A\r\n",
      " 19%|████████▎                                  | 11/57 [00:01<00:08,  5.31it/s]\u001B[A\r\n",
      " 21%|█████████                                  | 12/57 [00:02<00:08,  5.28it/s]\u001B[A\r\n",
      " 23%|█████████▊                                 | 13/57 [00:02<00:08,  5.21it/s]\u001B[A\r\n",
      " 25%|██████████▌                                | 14/57 [00:02<00:08,  5.17it/s]\u001B[A\r\n",
      " 26%|███████████▎                               | 15/57 [00:02<00:08,  5.14it/s]\u001B[A\r\n",
      " 28%|████████████                               | 16/57 [00:02<00:07,  5.13it/s]\u001B[A\r\n",
      " 30%|████████████▊                              | 17/57 [00:03<00:07,  5.10it/s]\u001B[A\r\n",
      " 32%|█████████████▌                             | 18/57 [00:03<00:07,  5.12it/s]\u001B[A\r\n",
      " 33%|██████████████▎                            | 19/57 [00:03<00:07,  5.15it/s]\u001B[A\r\n",
      " 35%|███████████████                            | 20/57 [00:03<00:07,  5.16it/s]\u001B[A\r\n",
      " 37%|███████████████▊                           | 21/57 [00:03<00:06,  5.18it/s]\u001B[A\r\n",
      " 39%|████████████████▌                          | 22/57 [00:04<00:06,  5.21it/s]\u001B[A\r\n",
      " 40%|█████████████████▎                         | 23/57 [00:04<00:06,  4.88it/s]\u001B[A\r\n",
      " 42%|██████████████████                         | 24/57 [00:04<00:06,  4.98it/s]\u001B[A\r\n",
      " 44%|██████████████████▊                        | 25/57 [00:04<00:06,  5.06it/s]\u001B[A\r\n",
      " 46%|███████████████████▌                       | 26/57 [00:04<00:06,  5.10it/s]\u001B[A\r\n",
      " 47%|████████████████████▎                      | 27/57 [00:05<00:05,  5.15it/s]\u001B[A\r\n",
      " 49%|█████████████████████                      | 28/57 [00:05<00:05,  5.13it/s]\u001B[A\r\n",
      " 51%|█████████████████████▉                     | 29/57 [00:05<00:05,  5.15it/s]\u001B[A\r\n",
      " 53%|██████████████████████▋                    | 30/57 [00:05<00:05,  5.15it/s]\u001B[A\r\n",
      " 54%|███████████████████████▍                   | 31/57 [00:05<00:05,  5.15it/s]\u001B[A\r\n",
      " 56%|████████████████████████▏                  | 32/57 [00:06<00:04,  5.11it/s]\u001B[A\r\n",
      " 58%|████████████████████████▉                  | 33/57 [00:06<00:04,  4.91it/s]\u001B[A\r\n",
      " 60%|█████████████████████████▋                 | 34/57 [00:06<00:04,  4.80it/s]\u001B[A\r\n",
      " 61%|██████████████████████████▍                | 35/57 [00:06<00:04,  4.80it/s]\u001B[A\r\n",
      " 63%|███████████████████████████▏               | 36/57 [00:06<00:04,  4.87it/s]\u001B[A\r\n",
      " 65%|███████████████████████████▉               | 37/57 [00:07<00:04,  4.86it/s]\u001B[A\r\n",
      " 67%|████████████████████████████▋              | 38/57 [00:07<00:03,  4.94it/s]\u001B[A\r\n",
      " 68%|█████████████████████████████▍             | 39/57 [00:07<00:03,  4.96it/s]\u001B[A\r\n",
      " 70%|██████████████████████████████▏            | 40/57 [00:07<00:03,  4.89it/s]\u001B[A\r\n",
      " 72%|██████████████████████████████▉            | 41/57 [00:07<00:03,  4.86it/s]\u001B[A\r\n",
      " 74%|███████████████████████████████▋           | 42/57 [00:08<00:03,  4.81it/s]\u001B[A\r\n",
      " 75%|████████████████████████████████▍          | 43/57 [00:08<00:02,  4.80it/s]\u001B[A\r\n",
      " 77%|█████████████████████████████████▏         | 44/57 [00:08<00:02,  4.79it/s]\u001B[A\r\n",
      " 79%|█████████████████████████████████▉         | 45/57 [00:08<00:02,  4.78it/s]\u001B[A\r\n",
      " 81%|██████████████████████████████████▋        | 46/57 [00:08<00:02,  4.79it/s]\u001B[A\r\n",
      " 82%|███████████████████████████████████▍       | 47/57 [00:09<00:02,  4.72it/s]\u001B[A\r\n",
      " 84%|████████████████████████████████████▏      | 48/57 [00:09<00:01,  4.71it/s]\u001B[A\r\n",
      " 86%|████████████████████████████████████▉      | 49/57 [00:09<00:01,  4.68it/s]\u001B[A\r\n",
      " 88%|█████████████████████████████████████▋     | 50/57 [00:09<00:01,  4.71it/s]\u001B[A\r\n",
      " 89%|██████████████████████████████████████▍    | 51/57 [00:10<00:01,  4.79it/s]\u001B[A\r\n",
      " 91%|███████████████████████████████████████▏   | 52/57 [00:10<00:01,  4.81it/s]\u001B[A\r\n",
      " 93%|███████████████████████████████████████▉   | 53/57 [00:10<00:00,  4.81it/s]\u001B[A\r\n",
      " 95%|████████████████████████████████████████▋  | 54/57 [00:10<00:00,  4.81it/s]\u001B[A\r\n",
      " 96%|█████████████████████████████████████████▍ | 55/57 [00:10<00:00,  4.83it/s]\u001B[A\r\n",
      " 98%|██████████████████████████████████████████▏| 56/57 [00:11<00:00,  4.79it/s]\u001B[A\r\n",
      "                                                                                \u001B[A\r\n",
      "\u001B[A{'eval_loss': 0.10568542778491974, 'eval_accuracy': 0.8675496688741722, 'eval_precision': 0.8710730295907132, 'eval_recall': 0.8675496688741722, 'eval_f1': 0.8664188429338998, 'eval_runtime': 16.9802, 'eval_samples_per_second': 26.678, 'eval_steps_per_second': 3.357, 'epoch': 1.0}\r\n",
      " 20%|████████                                | 254/1270 [04:43<16:40,  1.02it/s]\r\n",
      "100%|███████████████████████████████████████████| 57/57 [00:14<00:00,  1.08s/it]\u001B[A\r\n",
      "                                                                                \u001B[A/Users/wengbenjue/opt/anaconda3/envs/peft/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\r\n",
      "  warnings.warn(\r\n",
      "{'loss': 0.1688, 'learning_rate': 0.00015905511811023623, 'epoch': 1.02}        \r\n",
      "{'loss': 0.1467, 'learning_rate': 0.00015748031496062994, 'epoch': 1.06}        \r\n",
      "{'loss': 0.1727, 'learning_rate': 0.00015590551181102362, 'epoch': 1.1}         \r\n",
      "{'loss': 0.1074, 'learning_rate': 0.00015433070866141733, 'epoch': 1.14}        \r\n",
      "{'loss': 0.1665, 'learning_rate': 0.00015275590551181104, 'epoch': 1.18}        \r\n",
      " 24%|█████████▍                              | 300/1270 [14:51<16:37,  1.03s/it]/Users/wengbenjue/opt/anaconda3/envs/peft/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\r\n",
      "  warnings.warn(\r\n",
      "{'loss': 0.1478, 'learning_rate': 0.00015118110236220472, 'epoch': 1.22}        \r\n",
      "{'loss': 0.132, 'learning_rate': 0.00014960629921259843, 'epoch': 1.26}         \r\n",
      "{'loss': 0.159, 'learning_rate': 0.00014803149606299214, 'epoch': 1.3}          \r\n",
      "{'loss': 0.1572, 'learning_rate': 0.00014645669291338584, 'epoch': 1.33}        \r\n",
      "{'loss': 0.1315, 'learning_rate': 0.00014488188976377955, 'epoch': 1.37}        \r\n",
      "{'loss': 0.2025, 'learning_rate': 0.00014330708661417323, 'epoch': 1.41}        \r\n",
      "{'loss': 0.1381, 'learning_rate': 0.00014173228346456694, 'epoch': 1.45}        \r\n",
      "{'loss': 0.1699, 'learning_rate': 0.00014015748031496062, 'epoch': 1.49}        \r\n",
      "{'loss': 0.1226, 'learning_rate': 0.00013858267716535433, 'epoch': 1.53}        \r\n",
      "{'loss': 0.146, 'learning_rate': 0.00013700787401574804, 'epoch': 1.57}         \r\n",
      " 31%|████████████▌                           | 400/1270 [16:27<13:44,  1.05it/s]/Users/wengbenjue/opt/anaconda3/envs/peft/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\r\n",
      "  warnings.warn(\r\n",
      "{'loss': 0.121, 'learning_rate': 0.00013543307086614175, 'epoch': 1.61}         \r\n",
      "{'loss': 0.153, 'learning_rate': 0.00013385826771653546, 'epoch': 1.65}         \r\n",
      "{'loss': 0.1593, 'learning_rate': 0.00013228346456692914, 'epoch': 1.69}        \r\n",
      "{'loss': 0.1234, 'learning_rate': 0.00013070866141732282, 'epoch': 1.73}        \r\n",
      "{'loss': 0.1364, 'learning_rate': 0.00012913385826771653, 'epoch': 1.77}        \r\n",
      "{'loss': 0.1044, 'learning_rate': 0.00012755905511811023, 'epoch': 1.81}        \r\n",
      "{'loss': 0.136, 'learning_rate': 0.00012598425196850394, 'epoch': 1.84}         \r\n",
      "{'loss': 0.1181, 'learning_rate': 0.00012440944881889765, 'epoch': 1.88}        \r\n",
      "{'loss': 0.1129, 'learning_rate': 0.00012283464566929136, 'epoch': 1.92}        \r\n",
      "{'loss': 0.1565, 'learning_rate': 0.00012125984251968505, 'epoch': 1.96}        \r\n",
      " 39%|███████████████▋                        | 500/1270 [18:06<12:54,  1.01s/it]/Users/wengbenjue/opt/anaconda3/envs/peft/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\r\n",
      "  warnings.warn(\r\n",
      " 40%|████████████████                        | 509/1270 [18:15<13:13,  1.04s/it]\r\n",
      "  0%|                                                    | 0/57 [00:00<?, ?it/s]\u001B[A\r\n",
      "  5%|██▎                                         | 3/57 [00:00<00:02, 27.00it/s]\u001B[A\r\n",
      " 11%|████▋                                       | 6/57 [00:00<00:02, 20.59it/s]\u001B[A\r\n",
      " 16%|██████▉                                     | 9/57 [00:00<00:02, 18.93it/s]\u001B[A\r\n",
      " 19%|████████▎                                  | 11/57 [00:00<00:02, 18.36it/s]\u001B[A\r\n",
      " 23%|█████████▊                                 | 13/57 [00:00<00:02, 18.17it/s]\u001B[A\r\n",
      " 26%|███████████▎                               | 15/57 [00:00<00:02, 18.11it/s]\u001B[A\r\n",
      " 30%|████████████▊                              | 17/57 [00:00<00:02, 18.29it/s]\u001B[A\r\n",
      " 33%|██████████████▎                            | 19/57 [00:01<00:02, 18.33it/s]\u001B[A\r\n",
      " 37%|███████████████▊                           | 21/57 [00:01<00:01, 18.10it/s]\u001B[A\r\n",
      " 40%|█████████████████▎                         | 23/57 [00:01<00:02, 16.71it/s]\u001B[A\r\n",
      " 44%|██████████████████▊                        | 25/57 [00:01<00:01, 17.16it/s]\u001B[A\r\n",
      " 47%|████████████████████▎                      | 27/57 [00:01<00:01, 17.27it/s]\u001B[A\r\n",
      " 51%|█████████████████████▉                     | 29/57 [00:01<00:01, 17.45it/s]\u001B[A\r\n",
      " 54%|███████████████████████▍                   | 31/57 [00:01<00:01, 17.51it/s]\u001B[A\r\n",
      " 58%|████████████████████████▉                  | 33/57 [00:01<00:01, 17.66it/s]\u001B[A\r\n",
      " 61%|██████████████████████████▍                | 35/57 [00:01<00:01, 16.12it/s]\u001B[A\r\n",
      " 65%|███████████████████████████▉               | 37/57 [00:02<00:01, 16.26it/s]\u001B[A\r\n",
      " 68%|█████████████████████████████▍             | 39/57 [00:02<00:01, 16.41it/s]\u001B[A\r\n",
      " 72%|██████████████████████████████▉            | 41/57 [00:02<00:00, 16.58it/s]\u001B[A\r\n",
      " 75%|████████████████████████████████▍          | 43/57 [00:02<00:00, 16.81it/s]\u001B[A\r\n",
      " 79%|█████████████████████████████████▉         | 45/57 [00:02<00:00, 15.55it/s]\u001B[A\r\n",
      " 82%|███████████████████████████████████▍       | 47/57 [00:02<00:00, 16.15it/s]\u001B[A\r\n",
      " 86%|████████████████████████████████████▉      | 49/57 [00:02<00:00, 16.71it/s]\u001B[A\r\n",
      " 89%|██████████████████████████████████████▍    | 51/57 [00:02<00:00, 15.69it/s]\u001B[A\r\n",
      " 93%|███████████████████████████████████████▉   | 53/57 [00:03<00:00, 16.50it/s]\u001B[A\r\n",
      " 96%|█████████████████████████████████████████▍ | 55/57 [00:03<00:00, 17.29it/s]\u001B[A\r\n",
      "                                                                                \u001B[A\r\n",
      "\u001B[A{'eval_loss': 0.06622850894927979, 'eval_accuracy': 0.9359823399558499, 'eval_precision': 0.9354098013358855, 'eval_recall': 0.9359823399558499, 'eval_f1': 0.9351961888078638, 'eval_runtime': 3.4098, 'eval_samples_per_second': 132.852, 'eval_steps_per_second': 16.716, 'epoch': 2.0}\r\n",
      " 40%|████████████████                        | 509/1270 [18:19<13:13,  1.04s/it]\r\n",
      "100%|███████████████████████████████████████████| 57/57 [00:03<00:00, 16.19it/s]\u001B[A\r\n",
      "                                                                                \u001B[A/Users/wengbenjue/opt/anaconda3/envs/peft/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\r\n",
      "  warnings.warn(\r\n",
      "{'loss': 0.1005, 'learning_rate': 0.00011968503937007876, 'epoch': 2.0}         \r\n",
      "{'loss': 0.1275, 'learning_rate': 0.00011811023622047244, 'epoch': 2.04}        \r\n",
      "{'loss': 0.0908, 'learning_rate': 0.00011653543307086614, 'epoch': 2.08}        \r\n",
      "{'loss': 0.1162, 'learning_rate': 0.00011496062992125984, 'epoch': 2.12}        \r\n",
      "{'loss': 0.1455, 'learning_rate': 0.00011338582677165355, 'epoch': 2.16}        \r\n",
      "{'loss': 0.1246, 'learning_rate': 0.00011181102362204725, 'epoch': 2.2}         \r\n",
      "{'loss': 0.1265, 'learning_rate': 0.00011023622047244096, 'epoch': 2.24}        \r\n",
      "{'loss': 0.1179, 'learning_rate': 0.00010866141732283466, 'epoch': 2.28}        \r\n",
      "{'loss': 0.1407, 'learning_rate': 0.00010708661417322836, 'epoch': 2.32}        \r\n",
      "{'loss': 0.1326, 'learning_rate': 0.00010551181102362204, 'epoch': 2.36}        \r\n",
      " 47%|██████████████████▉                     | 600/1270 [19:48<10:37,  1.05it/s]/Users/wengbenjue/opt/anaconda3/envs/peft/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\r\n",
      "  warnings.warn(\r\n",
      "{'loss': 0.1066, 'learning_rate': 0.00010393700787401575, 'epoch': 2.39}        \r\n",
      "{'loss': 0.136, 'learning_rate': 0.00010236220472440946, 'epoch': 2.43}         \r\n",
      "{'loss': 0.1345, 'learning_rate': 0.00010078740157480315, 'epoch': 2.47}        \r\n",
      "{'loss': 0.1446, 'learning_rate': 9.921259842519686e-05, 'epoch': 2.51}         \r\n",
      "{'loss': 0.1117, 'learning_rate': 9.763779527559055e-05, 'epoch': 2.55}         \r\n",
      "{'loss': 0.0989, 'learning_rate': 9.606299212598425e-05, 'epoch': 2.59}         \r\n",
      "{'loss': 0.1371, 'learning_rate': 9.448818897637796e-05, 'epoch': 2.63}         \r\n",
      "{'loss': 0.0889, 'learning_rate': 9.291338582677166e-05, 'epoch': 2.67}         \r\n",
      "{'loss': 0.0851, 'learning_rate': 9.133858267716536e-05, 'epoch': 2.71}         \r\n",
      "{'loss': 0.1052, 'learning_rate': 8.976377952755905e-05, 'epoch': 2.75}         \r\n",
      " 55%|██████████████████████                  | 700/1270 [21:26<09:15,  1.03it/s]/Users/wengbenjue/opt/anaconda3/envs/peft/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\r\n",
      "  warnings.warn(\r\n",
      "{'loss': 0.091, 'learning_rate': 8.818897637795276e-05, 'epoch': 2.79}          \r\n",
      "{'loss': 0.0661, 'learning_rate': 8.661417322834646e-05, 'epoch': 2.83}         \r\n",
      "{'loss': 0.0886, 'learning_rate': 8.503937007874016e-05, 'epoch': 2.87}         \r\n",
      "{'loss': 0.0797, 'learning_rate': 8.346456692913386e-05, 'epoch': 2.9}          \r\n",
      "{'loss': 0.0918, 'learning_rate': 8.188976377952757e-05, 'epoch': 2.94}         \r\n",
      "{'loss': 0.0922, 'learning_rate': 8.031496062992126e-05, 'epoch': 2.98}         \r\n",
      " 60%|████████████████████████                | 764/1270 [22:32<08:53,  1.05s/it]\r\n",
      "  0%|                                                    | 0/57 [00:00<?, ?it/s]\u001B[A\r\n",
      "  5%|██▎                                         | 3/57 [00:00<00:01, 27.12it/s]\u001B[A\r\n",
      " 11%|████▋                                       | 6/57 [00:00<00:02, 19.58it/s]\u001B[A\r\n",
      " 16%|██████▉                                     | 9/57 [00:00<00:02, 18.42it/s]\u001B[A\r\n",
      " 19%|████████▎                                  | 11/57 [00:00<00:02, 17.85it/s]\u001B[A\r\n",
      " 23%|█████████▊                                 | 13/57 [00:00<00:02, 17.40it/s]\u001B[A\r\n",
      " 26%|███████████▎                               | 15/57 [00:00<00:02, 17.11it/s]\u001B[A\r\n",
      " 30%|████████████▊                              | 17/57 [00:00<00:02, 17.02it/s]\u001B[A\r\n",
      " 33%|██████████████▎                            | 19/57 [00:01<00:02, 17.01it/s]\u001B[A\r\n",
      " 37%|███████████████▊                           | 21/57 [00:01<00:02, 16.82it/s]\u001B[A\r\n",
      " 40%|█████████████████▎                         | 23/57 [00:01<00:02, 15.77it/s]\u001B[A\r\n",
      " 44%|██████████████████▊                        | 25/57 [00:01<00:01, 16.01it/s]\u001B[A\r\n",
      " 47%|████████████████████▎                      | 27/57 [00:01<00:01, 16.23it/s]\u001B[A\r\n",
      " 51%|█████████████████████▉                     | 29/57 [00:01<00:01, 16.39it/s]\u001B[A\r\n",
      " 54%|███████████████████████▍                   | 31/57 [00:01<00:01, 16.59it/s]\u001B[A\r\n",
      " 58%|████████████████████████▉                  | 33/57 [00:01<00:01, 16.83it/s]\u001B[A\r\n",
      " 61%|██████████████████████████▍                | 35/57 [00:02<00:01, 15.52it/s]\u001B[A\r\n",
      " 65%|███████████████████████████▉               | 37/57 [00:02<00:01, 15.85it/s]\u001B[A\r\n",
      " 68%|█████████████████████████████▍             | 39/57 [00:02<00:01, 15.95it/s]\u001B[A\r\n",
      " 72%|██████████████████████████████▉            | 41/57 [00:02<00:00, 16.06it/s]\u001B[A\r\n",
      " 75%|████████████████████████████████▍          | 43/57 [00:02<00:00, 16.26it/s]\u001B[A\r\n",
      " 79%|█████████████████████████████████▉         | 45/57 [00:02<00:00, 15.38it/s]\u001B[A\r\n",
      " 82%|███████████████████████████████████▍       | 47/57 [00:02<00:00, 15.74it/s]\u001B[A\r\n",
      " 86%|████████████████████████████████████▉      | 49/57 [00:02<00:00, 15.01it/s]\u001B[A\r\n",
      " 89%|██████████████████████████████████████▍    | 51/57 [00:03<00:00, 14.12it/s]\u001B[A\r\n",
      " 93%|███████████████████████████████████████▉   | 53/57 [00:03<00:00, 14.51it/s]\u001B[A\r\n",
      " 96%|█████████████████████████████████████████▍ | 55/57 [00:03<00:00, 14.90it/s]\u001B[A\r\n",
      "                                                                                \u001B[A\r\n",
      "\u001B[A{'eval_loss': 0.05371532589197159, 'eval_accuracy': 0.9426048565121413, 'eval_precision': 0.942173514775385, 'eval_recall': 0.9426048565121413, 'eval_f1': 0.9419925877921829, 'eval_runtime': 3.6519, 'eval_samples_per_second': 124.046, 'eval_steps_per_second': 15.608, 'epoch': 3.0}\r\n",
      " 60%|████████████████████████                | 764/1270 [22:36<08:53,  1.05s/it]\r\n",
      "100%|███████████████████████████████████████████| 57/57 [00:03<00:00, 14.12it/s]\u001B[A\r\n",
      "                                                                                \u001B[A/Users/wengbenjue/opt/anaconda3/envs/peft/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\r\n",
      "  warnings.warn(\r\n",
      "{'loss': 0.1036, 'learning_rate': 7.874015748031497e-05, 'epoch': 3.02}         \r\n",
      "{'loss': 0.1065, 'learning_rate': 7.716535433070867e-05, 'epoch': 3.06}         \r\n",
      "{'loss': 0.0949, 'learning_rate': 7.559055118110236e-05, 'epoch': 3.1}          \r\n",
      "{'loss': 0.1039, 'learning_rate': 7.401574803149607e-05, 'epoch': 3.14}         \r\n",
      " 63%|█████████████████████████▏              | 800/1270 [23:13<08:05,  1.03s/it]/Users/wengbenjue/opt/anaconda3/envs/peft/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\r\n",
      "  warnings.warn(\r\n",
      "{'loss': 0.1018, 'learning_rate': 7.244094488188978e-05, 'epoch': 3.18}         \r\n",
      "{'loss': 0.0797, 'learning_rate': 7.086614173228347e-05, 'epoch': 3.22}         \r\n",
      "{'loss': 0.1214, 'learning_rate': 6.929133858267717e-05, 'epoch': 3.26}         \r\n",
      "{'loss': 0.0937, 'learning_rate': 6.771653543307087e-05, 'epoch': 3.3}          \r\n",
      "{'loss': 0.0969, 'learning_rate': 6.614173228346457e-05, 'epoch': 3.34}         \r\n",
      "{'loss': 0.1396, 'learning_rate': 6.456692913385826e-05, 'epoch': 3.38}         \r\n",
      "{'loss': 0.0712, 'learning_rate': 6.299212598425197e-05, 'epoch': 3.42}         \r\n",
      "{'loss': 0.1048, 'learning_rate': 6.141732283464568e-05, 'epoch': 3.45}         \r\n",
      "{'loss': 0.0942, 'learning_rate': 5.984251968503938e-05, 'epoch': 3.49}         \r\n",
      "{'loss': 0.0779, 'learning_rate': 5.826771653543307e-05, 'epoch': 3.53}         \r\n",
      " 71%|████████████████████████████▎           | 900/1270 [24:58<06:24,  1.04s/it]/Users/wengbenjue/opt/anaconda3/envs/peft/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\r\n",
      "  warnings.warn(\r\n",
      "{'loss': 0.0974, 'learning_rate': 5.6692913385826777e-05, 'epoch': 3.57}        \r\n",
      "{'loss': 0.0727, 'learning_rate': 5.511811023622048e-05, 'epoch': 3.61}         \r\n",
      "{'loss': 0.1092, 'learning_rate': 5.354330708661418e-05, 'epoch': 3.65}         \r\n",
      "{'loss': 0.1144, 'learning_rate': 5.1968503937007874e-05, 'epoch': 3.69}        \r\n",
      "{'loss': 0.1228, 'learning_rate': 5.0393700787401575e-05, 'epoch': 3.73}        \r\n",
      "{'loss': 0.1075, 'learning_rate': 4.881889763779528e-05, 'epoch': 3.77}         \r\n",
      "{'loss': 0.0871, 'learning_rate': 4.724409448818898e-05, 'epoch': 3.81}         \r\n",
      "{'loss': 0.0927, 'learning_rate': 4.566929133858268e-05, 'epoch': 3.85}         \r\n",
      "{'loss': 0.0959, 'learning_rate': 4.409448818897638e-05, 'epoch': 3.89}         \r\n",
      "{'loss': 0.0904, 'learning_rate': 4.251968503937008e-05, 'epoch': 3.93}         \r\n",
      " 79%|██████████████████████████████▋        | 1000/1270 [26:42<04:45,  1.06s/it]/Users/wengbenjue/opt/anaconda3/envs/peft/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\r\n",
      "  warnings.warn(\r\n",
      "{'loss': 0.1034, 'learning_rate': 4.0944881889763784e-05, 'epoch': 3.96}        \r\n",
      " 80%|███████████████████████████████▎       | 1019/1270 [27:02<04:17,  1.02s/it]\r\n",
      "  0%|                                                    | 0/57 [00:00<?, ?it/s]\u001B[A\r\n",
      "  5%|██▎                                         | 3/57 [00:00<00:02, 26.91it/s]\u001B[A\r\n",
      " 11%|████▋                                       | 6/57 [00:00<00:02, 20.61it/s]\u001B[A\r\n",
      " 16%|██████▉                                     | 9/57 [00:00<00:02, 19.41it/s]\u001B[A\r\n",
      " 19%|████████▎                                  | 11/57 [00:00<00:02, 18.99it/s]\u001B[A\r\n",
      " 23%|█████████▊                                 | 13/57 [00:00<00:02, 18.73it/s]\u001B[A\r\n",
      " 26%|███████████▎                               | 15/57 [00:00<00:02, 18.52it/s]\u001B[A\r\n",
      " 30%|████████████▊                              | 17/57 [00:00<00:02, 18.37it/s]\u001B[A\r\n",
      " 33%|██████████████▎                            | 19/57 [00:00<00:02, 18.35it/s]\u001B[A\r\n",
      " 37%|███████████████▊                           | 21/57 [00:01<00:01, 18.08it/s]\u001B[A\r\n",
      " 40%|█████████████████▎                         | 23/57 [00:01<00:02, 16.69it/s]\u001B[A\r\n",
      " 44%|██████████████████▊                        | 25/57 [00:01<00:01, 16.85it/s]\u001B[A\r\n",
      " 47%|████████████████████▎                      | 27/57 [00:01<00:01, 16.83it/s]\u001B[A\r\n",
      " 51%|█████████████████████▉                     | 29/57 [00:01<00:01, 16.79it/s]\u001B[A\r\n",
      " 54%|███████████████████████▍                   | 31/57 [00:01<00:01, 16.80it/s]\u001B[A\r\n",
      " 58%|████████████████████████▉                  | 33/57 [00:01<00:01, 16.72it/s]\u001B[A\r\n",
      " 61%|██████████████████████████▍                | 35/57 [00:01<00:01, 15.76it/s]\u001B[A\r\n",
      " 65%|███████████████████████████▉               | 37/57 [00:02<00:01, 16.00it/s]\u001B[A\r\n",
      " 68%|█████████████████████████████▍             | 39/57 [00:02<00:01, 16.23it/s]\u001B[A\r\n",
      " 72%|██████████████████████████████▉            | 41/57 [00:02<00:00, 16.37it/s]\u001B[A\r\n",
      " 75%|████████████████████████████████▍          | 43/57 [00:02<00:00, 16.30it/s]\u001B[A\r\n",
      " 79%|█████████████████████████████████▉         | 45/57 [00:02<00:00, 15.50it/s]\u001B[A\r\n",
      " 82%|███████████████████████████████████▍       | 47/57 [00:02<00:00, 15.84it/s]\u001B[A\r\n",
      " 86%|████████████████████████████████████▉      | 49/57 [00:02<00:00, 16.04it/s]\u001B[A\r\n",
      " 89%|██████████████████████████████████████▍    | 51/57 [00:03<00:00, 15.29it/s]\u001B[A\r\n",
      " 93%|███████████████████████████████████████▉   | 53/57 [00:03<00:00, 15.85it/s]\u001B[A\r\n",
      " 96%|█████████████████████████████████████████▍ | 55/57 [00:03<00:00, 16.07it/s]\u001B[A\r\n",
      "                                                                                \u001B[A\r\n",
      "\u001B[A{'eval_loss': 0.049839086830616, 'eval_accuracy': 0.9514348785871964, 'eval_precision': 0.9521294022556639, 'eval_recall': 0.9514348785871964, 'eval_f1': 0.9504822296436425, 'eval_runtime': 3.4782, 'eval_samples_per_second': 130.241, 'eval_steps_per_second': 16.388, 'epoch': 4.0}\r\n",
      " 80%|███████████████████████████████▎       | 1019/1270 [27:06<04:17,  1.02s/it]\r\n",
      "100%|███████████████████████████████████████████| 57/57 [00:03<00:00, 15.31it/s]\u001B[A\r\n",
      "                                                                                \u001B[A/Users/wengbenjue/opt/anaconda3/envs/peft/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\r\n",
      "  warnings.warn(\r\n",
      "{'loss': 0.0675, 'learning_rate': 3.9370078740157485e-05, 'epoch': 4.0}         \r\n",
      "{'loss': 0.0753, 'learning_rate': 3.779527559055118e-05, 'epoch': 4.04}         \r\n",
      "{'loss': 0.0761, 'learning_rate': 3.622047244094489e-05, 'epoch': 4.08}         \r\n",
      "{'loss': 0.0793, 'learning_rate': 3.464566929133858e-05, 'epoch': 4.12}         \r\n",
      "{'loss': 0.0919, 'learning_rate': 3.3070866141732284e-05, 'epoch': 4.16}        \r\n",
      "{'loss': 0.0747, 'learning_rate': 3.1496062992125985e-05, 'epoch': 4.2}         \r\n",
      "{'loss': 0.0965, 'learning_rate': 2.992125984251969e-05, 'epoch': 4.24}         \r\n",
      "{'loss': 0.1298, 'learning_rate': 2.8346456692913388e-05, 'epoch': 4.28}        \r\n",
      "{'loss': 0.0599, 'learning_rate': 2.677165354330709e-05, 'epoch': 4.32}         \r\n",
      " 87%|█████████████████████████████████▊     | 1100/1270 [28:31<02:57,  1.05s/it]/Users/wengbenjue/opt/anaconda3/envs/peft/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\r\n",
      "  warnings.warn(\r\n",
      "{'loss': 0.0778, 'learning_rate': 2.5196850393700788e-05, 'epoch': 4.36}        \r\n",
      "{'loss': 0.0971, 'learning_rate': 2.362204724409449e-05, 'epoch': 4.4}          \r\n",
      "{'loss': 0.0527, 'learning_rate': 2.204724409448819e-05, 'epoch': 4.44}         \r\n",
      "{'loss': 0.0897, 'learning_rate': 2.0472440944881892e-05, 'epoch': 4.47}        \r\n",
      "{'loss': 0.0813, 'learning_rate': 1.889763779527559e-05, 'epoch': 4.51}         \r\n",
      "{'loss': 0.0975, 'learning_rate': 1.732283464566929e-05, 'epoch': 4.55}         \r\n",
      "{'loss': 0.1103, 'learning_rate': 1.5748031496062993e-05, 'epoch': 4.59}        \r\n",
      "{'loss': 0.0826, 'learning_rate': 1.4173228346456694e-05, 'epoch': 4.63}        \r\n",
      "{'loss': 0.0778, 'learning_rate': 1.2598425196850394e-05, 'epoch': 4.67}        \r\n",
      "{'loss': 0.0857, 'learning_rate': 1.1023622047244095e-05, 'epoch': 4.71}        \r\n",
      " 94%|████████████████████████████████████▊  | 1200/1270 [30:15<01:11,  1.03s/it]/Users/wengbenjue/opt/anaconda3/envs/peft/lib/python3.11/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\r\n",
      "  warnings.warn(\r\n",
      "{'loss': 0.0854, 'learning_rate': 9.448818897637795e-06, 'epoch': 4.75}         \r\n",
      "{'loss': 0.0689, 'learning_rate': 7.874015748031496e-06, 'epoch': 4.79}         \r\n",
      "{'loss': 0.1158, 'learning_rate': 6.299212598425197e-06, 'epoch': 4.83}         \r\n",
      "{'loss': 0.1584, 'learning_rate': 4.7244094488188975e-06, 'epoch': 4.87}        \r\n",
      "{'loss': 0.0739, 'learning_rate': 3.1496062992125985e-06, 'epoch': 4.91}        \r\n",
      "{'loss': 0.132, 'learning_rate': 1.5748031496062992e-06, 'epoch': 4.95}         \r\n",
      "{'loss': 0.0922, 'learning_rate': 0.0, 'epoch': 4.99}                           \r\n",
      "100%|███████████████████████████████████████| 1270/1270 [31:27<00:00,  1.06s/it]\r\n",
      "  0%|                                                    | 0/57 [00:00<?, ?it/s]\u001B[A\r\n",
      "  5%|██▎                                         | 3/57 [00:00<00:02, 23.58it/s]\u001B[A\r\n",
      " 11%|████▋                                       | 6/57 [00:00<00:02, 19.29it/s]\u001B[A\r\n",
      " 14%|██████▏                                     | 8/57 [00:00<00:02, 18.66it/s]\u001B[A\r\n",
      " 18%|███████▌                                   | 10/57 [00:00<00:02, 18.09it/s]\u001B[A\r\n",
      " 21%|█████████                                  | 12/57 [00:00<00:02, 17.80it/s]\u001B[A\r\n",
      " 25%|██████████▌                                | 14/57 [00:00<00:02, 17.66it/s]\u001B[A\r\n",
      " 28%|████████████                               | 16/57 [00:00<00:02, 17.63it/s]\u001B[A\r\n",
      " 32%|█████████████▌                             | 18/57 [00:00<00:02, 17.69it/s]\u001B[A\r\n",
      " 35%|███████████████                            | 20/57 [00:01<00:02, 17.81it/s]\u001B[A\r\n",
      " 39%|████████████████▌                          | 22/57 [00:01<00:01, 17.69it/s]\u001B[A\r\n",
      " 42%|██████████████████                         | 24/57 [00:01<00:01, 16.57it/s]\u001B[A\r\n",
      " 46%|███████████████████▌                       | 26/57 [00:01<00:01, 16.74it/s]\u001B[A\r\n",
      " 49%|█████████████████████                      | 28/57 [00:01<00:01, 16.99it/s]\u001B[A\r\n",
      " 53%|██████████████████████▋                    | 30/57 [00:01<00:01, 17.04it/s]\u001B[A\r\n",
      " 56%|████████████████████████▏                  | 32/57 [00:01<00:01, 17.03it/s]\u001B[A\r\n",
      " 60%|█████████████████████████▋                 | 34/57 [00:01<00:01, 17.16it/s]\u001B[A\r\n",
      " 63%|███████████████████████████▏               | 36/57 [00:02<00:01, 15.93it/s]\u001B[A\r\n",
      " 67%|████████████████████████████▋              | 38/57 [00:02<00:01, 16.26it/s]\u001B[A\r\n",
      " 70%|██████████████████████████████▏            | 40/57 [00:02<00:01, 16.41it/s]\u001B[A\r\n",
      " 74%|███████████████████████████████▋           | 42/57 [00:02<00:00, 16.43it/s]\u001B[A\r\n",
      " 77%|█████████████████████████████████▏         | 44/57 [00:02<00:00, 15.80it/s]\u001B[A\r\n",
      " 81%|██████████████████████████████████▋        | 46/57 [00:02<00:00, 15.91it/s]\u001B[A\r\n",
      " 84%|████████████████████████████████████▏      | 48/57 [00:02<00:00, 16.10it/s]\u001B[A\r\n",
      " 88%|█████████████████████████████████████▋     | 50/57 [00:02<00:00, 16.16it/s]\u001B[A\r\n",
      " 91%|███████████████████████████████████████▏   | 52/57 [00:03<00:00, 15.53it/s]\u001B[A\r\n",
      " 95%|████████████████████████████████████████▋  | 54/57 [00:03<00:00, 15.87it/s]\u001B[A\r\n",
      "                                                                                \r[A\r\n",
      "\u001B[A{'eval_loss': 0.04788612946867943, 'eval_accuracy': 0.9492273730684326, 'eval_precision': 0.949037233506364, 'eval_recall': 0.9492273730684326, 'eval_f1': 0.9486194437300349, 'eval_runtime': 3.4928, 'eval_samples_per_second': 129.696, 'eval_steps_per_second': 16.319, 'epoch': 4.99}\r\n",
      "100%|███████████████████████████████████████| 1270/1270 [31:31<00:00,  1.06s/it]\r\n",
      "100%|███████████████████████████████████████████| 57/57 [00:03<00:00, 16.25it/s]\u001B[A\r\n",
      "{'train_runtime': 1897.0856, 'train_samples_per_second': 10.74, 'train_steps_per_second': 0.669, 'train_loss': 0.3055350720882416, 'epoch': 4.99}\r\n",
      "100%|███████████████████████████████████████| 1270/1270 [31:31<00:00,  1.49s/it]\r\n"
     ]
    }
   ],
   "source": [
    "!python ../../../peft_train.py \\\n",
    "--model_name ../../../pretrain_models/flan-t5-xl \\\n",
    "--max_seq_len 128 \\\n",
    "--group_by_length \\\n",
    "--max_steps 200 \\\n",
    "--dataset_name ../../../text-classification/imdb \\\n",
    "--num_labels 2 \\\n",
    "--epochs 5 \\\n",
    "--learning_rate 1e-3 \\\n",
    "--per_device_train_batch_size 16 \\\n",
    "--per_device_eval_batch_size 16 \\\n",
    "--model_type SEQ_2_SEQ_LM \\\n",
    "--output_model_path ./result/flan-t5-xl-imdb-lora \\\n",
    "--bnb_4bit_compute_dtype float16 \\\n",
    "--use_4b False \\\n",
    "--use_cpu\n"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1.2.2 Run code on GPU version\n",
    "Load the model together with the adapter with few lines of code! Check the snippet below to load the adapter from the Hub and run the example evaluation."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!pip install -q -U trl transformers accelerate git+https://github.com/huggingface/peft.git\n",
    "!pip install -q datasets bitsandbytes einops wandb evaluate\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "%cd /content/drive/MyDrive/Colab Notebooks/llms-peft-cook-colab/experiments/flan-t5-xl-lora/imdb"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "!python ../../../peft_train.py \\\n",
    "--model_name google/flan-t5-xl \\\n",
    "--max_seq_len 2048 \\\n",
    "--group_by_length \\\n",
    "--max_steps 200 \\\n",
    "--dataset_name ../../../text-classification/imdb \\\n",
    "--num_labels 2 \\\n",
    "--epochs 5 \\\n",
    "--learning_rate 1e-3 \\\n",
    "--per_device_train_batch_size 64 \\\n",
    "--per_device_eval_batch_size 64 \\\n",
    "--model_type SEQ_2_SEQ_LM \\\n",
    "--output_model_path ./result/flan-t5-xl-imdb-lora \\\n",
    "--bnb_4bit_compute_dtype float16 \\\n",
    "--load_in_8bit \\\n",
    "--use_4b"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Load your adapter from the Hub"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "import torch\n",
    "from peft import PeftModel, PeftConfig\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "peft_model_id = \"./result/flan-t5-cup-lora\"\n",
    "base_model_name_or_path = '.../../../pretrain_models/google-flan-t5-small'\n",
    "config = PeftConfig.from_pretrained(peft_model_id)\n",
    "\n",
    "# model = AutoModelForSeq2SeqLM.from_pretrained(config.base_model_name_or_path, torch_dtype=\"auto\", device_map=\"auto\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(config.base_model_name_or_path, torch_dtype=\"auto\", device_map=\"cpu\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path)\n",
    "\n",
    "# Load the Lora model\n",
    "model = PeftModel.from_pretrained(model, peft_model_id)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-02T16:37:01.962850Z",
     "start_time": "2023-12-02T16:37:01.696535Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input sentence:  In January-September 2009 , the Group 's net interest income increased to EUR 112.4 mn from EUR 74.3 mn in January-September 2008 .\n",
      " output prediction:  ['positive positive positive positive positive positive positive positive positive positive']\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "input_text = \"In January-September 2009 , the Group 's net interest income increased to EUR 112.4 mn from EUR 74.3 mn in January-September 2008 .\"\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
    "\n",
    "outputs = model.generate(input_ids=inputs[\"input_ids\"], max_new_tokens=10)\n",
    "\n",
    "print(\"input sentence: \", input_text)\n",
    "print(\" output prediction: \", tokenizer.batch_decode(outputs.detach().cpu().numpy(), skip_special_tokens=True))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-02T16:40:55.202266Z",
     "start_time": "2023-12-02T16:40:55.100686Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input sentence:  In January-September 2009 , the Group 's net interest income increased to EUR 112.4 mn from EUR 74.3 mn in January-September 2008 .\n",
      " output prediction:  ['positive positive positive positive positive positive positive positive positive positive']\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "input_text = \"In January-September 2009 , the Group 's net interest income increased to EUR 112.4 mn from EUR 74.3 mn in January-September 2008 .\"\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
    "\n",
    "outputs = model.generate(input_ids=inputs[\"input_ids\"], max_new_tokens=10)\n",
    "\n",
    "print(\"input sentence: \", input_text)\n",
    "print(\" output prediction: \", tokenizer.batch_decode(outputs.detach().cpu().numpy(), skip_special_tokens=True))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-02T16:37:04.919570Z",
     "start_time": "2023-12-02T16:37:04.379496Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1.3 Experimental Result"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class HTMLRender:\n",
    "    def __init__(self,html_str):\n",
    "        self.html_str =html_str\n",
    "    def _repr_html_(self):\n",
    "       return self.html_str\n"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model_accurcy_html = '''\n",
    "<table>\n",
    "  <tr>\n",
    "    <th>eval loss</th>\n",
    "    <th>eval accuracy</th>\n",
    "    <th>eval precision</th>\n",
    "    <th>eval recall</th>\n",
    "    <th>eval f1</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td style=\"background-color:#4C72B0;\">0.102</td>\n",
    "    <td style=\"background-color:#55A868;\">0.896</td>\n",
    "    <td style=\"background-color:#C44E52;\">0.90</td>\n",
    "     <td style=\"background-color:#8172B2;\">0.896</td>\n",
    "    <td style=\"background-color:#64B5CD;\">0.894</td>\n",
    "  </tr>\n",
    "\n",
    "</table>\n",
    "\n",
    "       '''\n",
    "HTMLRender(model_accurcy_html)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "accuracy_html = '''\n",
    "<img src=\"./image/flan-t5-small-accuracy.png\" alt=\"flan-t5-small-accuracy\" width=\"70%\">\n",
    "'''\n",
    "HTMLRender(accuracy_html)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "## Load your adapter from the Hub\n"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
